{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3671c5",
   "metadata": {},
   "source": [
    "# Simple bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "697de0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bag of words \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc57b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [\"I love the book\", \"This is a great book\", \"the fit is great\", \"I love the shoses\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "840299a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec061a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book', 'fit', 'great', 'is', 'love', 'shoses', 'the', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so this will give you all the words(features) - removing some words it doesn't want\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "85aaed0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.toarray()\n",
    "# the first array item represents the first item in the training data\n",
    "# one means the feature exists in the item, zero means non-exist\n",
    "# so the first one is saying \"book\", \"love\", \"the\" are in the first array item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e32618e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "train_x = [\"I love the book\", \"This is a great book\", \"the fit is great\", \"I love the shoses\"]\n",
    "\n",
    "class Category:\n",
    "    BOOKS = \"BOOKS\"\n",
    "    CLOTHING = \"CLOTHING\"\n",
    "\n",
    "# y train\n",
    "train_y = [Category.BOOKS, Category.BOOKS, Category.CLOTHING, Category.CLOTHING]\n",
    "\n",
    "# here use ngram and specify both 1 word and 2 words\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1,2))\n",
    "# now you have the word in numeric values\n",
    "train_x_vectors = vectorizer.fit_transform(train_x)\n",
    "\n",
    "clf_svm = svm.SVC(kernel=\"linear\")\n",
    "clf_svm.fit(train_x_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2fb49447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to predict \n",
    "\n",
    "test_x_vectors = vectorizer.transform([\"Good book\"])\n",
    "test_x_vectors.toarray()  # the first element is  the word \"book\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e386e7",
   "metadata": {},
   "source": [
    "# Word Vectors model - sPacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "688ee19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e1beb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "883e2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "622a16b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I love books"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"I love books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00c0488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this is a good book"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"this is a good book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1185c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love the book', 'This is a great book', 'the fit is great', 'I love the shoses']\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0694da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(txt) for txt in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44671e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeding representation\n",
    "#docs[0].vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1f0b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_word_vectors = [x.vector for x in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "355ed9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1211e3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now use the word embeding for training \n",
    "clf_svm_wv = svm.SVC(kernel=\"linear\")\n",
    "clf_svm_wv.fit(train_x_word_vectors, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72b20da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHING', 'CLOTHING'], dtype='<U8')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Steps\n",
    "1. define your texts in an array\n",
    "2. transform them into nlp docs\n",
    "3. get the word embeding vectors\n",
    "\"\"\"\n",
    "\n",
    "test_x = [\"I went to the bank and wrote a check\", \"let me check that out\"]\n",
    "test_docs = [nlp(t) for t in test_x]\n",
    "test_x_word_vectors = [x.vector for x in test_docs]\n",
    "\n",
    "clf_svm_wv.predict(test_x_word_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6043a8",
   "metadata": {},
   "source": [
    "## More SpaCy Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa2c8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e434c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x1324e5820>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3d84bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_doc = nlp(\"This is an introduction of SpaCy. Yeah it's pretty cool. I am detailed-oriented\")\n",
    "type(nlp_doc)  # Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4e1b0418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "an\n",
      "introduction\n",
      "of\n",
      "SpaCy\n",
      ".\n",
      "Yeah\n",
      "it\n",
      "'s\n",
      "pretty\n",
      "cool\n",
      ".\n",
      "I\n",
      "am\n",
      "detailed\n",
      "-\n",
      "oriented\n"
     ]
    }
   ],
   "source": [
    "# already tokenized\n",
    "\n",
    "for token in nlp_doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "065aea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an introduction of SpaCy.\n",
      "Yeah it's pretty cool.\n",
      "I am detailed-oriented\n"
     ]
    }
   ],
   "source": [
    "for sentence in nlp_doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c8e8dee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Alphabeti?  True\n",
      "This Punctuation?:  False\n",
      "This Stop word?:  True\n",
      "is Alphabeti?  True\n",
      "is Punctuation?:  False\n",
      "is Stop word?:  True\n",
      "an Alphabeti?  True\n",
      "an Punctuation?:  False\n",
      "an Stop word?:  True\n",
      "introduction Alphabeti?  True\n",
      "introduction Punctuation?:  False\n",
      "introduction Stop word?:  False\n",
      "of Alphabeti?  True\n",
      "of Punctuation?:  False\n",
      "of Stop word?:  True\n",
      "SpaCy Alphabeti?  True\n",
      "SpaCy Punctuation?:  False\n",
      "SpaCy Stop word?:  False\n",
      ". Alphabeti?  False\n",
      ". Punctuation?:  True\n",
      ". Stop word?:  False\n",
      "Yeah Alphabeti?  True\n",
      "Yeah Punctuation?:  False\n",
      "Yeah Stop word?:  False\n",
      "it Alphabeti?  True\n",
      "it Punctuation?:  False\n",
      "it Stop word?:  True\n",
      "'s Alphabeti?  False\n",
      "'s Punctuation?:  False\n",
      "'s Stop word?:  True\n",
      "pretty Alphabeti?  True\n",
      "pretty Punctuation?:  False\n",
      "pretty Stop word?:  False\n",
      "cool Alphabeti?  True\n",
      "cool Punctuation?:  False\n",
      "cool Stop word?:  False\n",
      ". Alphabeti?  False\n",
      ". Punctuation?:  True\n",
      ". Stop word?:  False\n",
      "I Alphabeti?  True\n",
      "I Punctuation?:  False\n",
      "I Stop word?:  True\n",
      "am Alphabeti?  True\n",
      "am Punctuation?:  False\n",
      "am Stop word?:  True\n",
      "detailed Alphabeti?  True\n",
      "detailed Punctuation?:  False\n",
      "detailed Stop word?:  False\n",
      "- Alphabeti?  False\n",
      "- Punctuation?:  True\n",
      "- Stop word?:  False\n",
      "oriented Alphabeti?  True\n",
      "oriented Punctuation?:  False\n",
      "oriented Stop word?:  False\n"
     ]
    }
   ],
   "source": [
    "# you can inspect the token \n",
    "for token in nlp_doc:\n",
    "    print(token,\"Alphabeti? \", token.is_alpha)\n",
    "    print(token,\"Punctuation?: \", token.is_punct)\n",
    "    print(token,\"Stop word?: \", token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f5f9d52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3af13a42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'because',\n",
       " 'mine',\n",
       " 'across',\n",
       " 'around',\n",
       " 'also',\n",
       " 'however',\n",
       " 'hers',\n",
       " 'thereby',\n",
       " 'former']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_stopwords)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36bffbc",
   "metadata": {},
   "source": [
    "### Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "595899d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"Hi I am Shin Hsu. Google LLC is an American multinational technology company focusing on search engine technology, online advertising, cloud computing, computer software, quantum computing, e-commerce, artificial intelligence, and consumer electronics.\"\"\"\n",
    "google_txt_doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8c631aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shin Hsu : PERSON  e.g.,People, including fictional\n",
      "Google LLC : ORG  e.g.,Companies, agencies, institutions, etc.\n",
      "American : NORP  e.g.,Nationalities or religious or political groups\n",
      "quantum computing : ORG  e.g.,Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Get the name entities using doc.ents\n",
    "for entity in google_txt_doc.ents:\n",
    "    print(entity.text,\":\", entity.label_, f\" e.g.,{spacy.explain(entity.label_)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "283dde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retokenize - we can merge some tokens to make them one token, E.g., first name + last name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9dbc6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(New Hampshire,)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"She lived in New Hampshire.\")\n",
    "print(doc.ents)\n",
    "\n",
    "[(token.text, token.i) for token in doc]\n",
    "[('She', 0), ('lived', 1), ('in', 2), ('New', 3), ('Hampshire', 4), ('.', 5)]\n",
    "print(len(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8342aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She \n",
      "lived \n",
      "in \n",
      "New Hampshire.\n"
     ]
    }
   ],
   "source": [
    "# use retokenize to merge certain tokens that you want to treat as one token\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(doc[3:5],attrs={\"LEMMA\": \"new hampshire\"})\n",
    "\n",
    "for t in doc:\n",
    "    print(t.text_with_ws) # now \"New Hampshre is treated as one token rather than two\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9a543486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shin \n",
      "Hsu\n"
     ]
    }
   ],
   "source": [
    "# back to the google text\n",
    "for token in google_txt_doc:\n",
    "    if token.ent_type_ == \"PERSON\":\n",
    "        print(token.text_with_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "cf26a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "I\n",
      "am\n",
      "Shin Hsu\n",
      ".\n",
      "Google LLC\n",
      "is\n",
      "an\n",
      "American\n",
      "multinational\n",
      "technology\n",
      "company\n",
      "focusing\n",
      "on\n",
      "search\n",
      "engine\n",
      "technology\n",
      ",\n",
      "online\n",
      "advertising\n",
      ",\n",
      "cloud\n",
      "computing\n",
      ",\n",
      "computer\n",
      "software\n",
      ",\n",
      "quantum computing\n",
      ",\n",
      "e\n",
      "-\n",
      "commerce\n",
      ",\n",
      "artificial\n",
      "intelligence\n",
      ",\n",
      "and\n",
      "consumer\n",
      "electronics\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# now back to the google text\n",
    "with google_txt_doc.retokenize() as retokenizer:\n",
    "    for entity in google_txt_doc.ents:\n",
    "        retokenizer.merge(entity)  # by doing this, now we can see all entities are now treaded as one token including person name (first name + last name)\n",
    "for t in google_txt_doc:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638e4df",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3811dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
